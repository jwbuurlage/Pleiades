/*
 * TODO Global to local indices
 * TODO Remote indices per scanline for each target
 */

#include <random>
#include <vector>

#include <bulk/bulk.hpp>

namespace pleiades {

std::size_t global_to_local(int rank, std::size_t global, geometry g) {
    return 0;
}

// a scanlines is a consecutive subarray from position `begin` up to `begin +
// count`
struct scanline {
    std::size_t begin;
    std::size_t count;
};

// a face has a list of contributing processors, and a list of scanlines
struct face {
    std::vector<int> contributors;
    std::vector<scanline> scanlines;
};

// for the gather step, one of the contributors of a face is considered its
// owner. the scanlines in `lines` are to be communicated to the owner's
// 'reduction buffer' at the index indicated by the first component of the pair
struct gather_task {
    int owner;
    std::vector<std::pair<std::size_t, scanline>> lines;
};

// for the scatter step, the owner communicates the reduction result back to the
// contributors. Each line has an associated tag, which is a list of remote
// indices of the contributors in their main data buffer
struct scatter_task {
    std::vector<int> contributors;
    std::vector<std::pair<std::vector<std::size_t>, scanline>> lines;
};

template <typename T>
void gather(bulk::coarray<T> buffer, std::vector<gather_task> tasks,
            const T* data) {
    for (auto task : tasks) {
        for (auto [remote, line] : task.lines) {
            auto [begin, count] = line;
            buffer(task.owner)[{remote, remote + count}] = {&data[begin],
                                                            count};
        }
    }
    buffer.world.sync();
}

template <typename T>
void scatter(bulk::coarray<T> buffer, std::vector<scatter_task> tasks,
             const T* data) {
    for (auto task : tasks) {
        for (auto [remotes, line] : task.lines) {
            for (auto i = 0; i < task.contributors.size(); ++i) {
                auto remote = remotes[i];
                buffer(task.contributors[i])[{remote, remote + count}] = {
                    &data[begin], count};
            }
        }
        buffer.world.sync();
    }
}

/** Outputs the gather and scatter tasks for the local processor */
template <typename T>
std::pair<std::vector<gather_task>, std::vector<scatter_task>>
tasks(bulk::world& world, const tpt::geometry::base<3_D, T>& g,
      const tpt::grcb::node<T>& root, tpt::grcb::cube<T> v) {
    // alias local rank and number of processors
    auto s = world.rank();
    auto p = world.active_processors();

    // TODO how to deal with the dynamic buffer size? I think we require another
    // synchronization step

    // The strategy is as follows.
    // 1. Assign the projections round robin, and treat them independently. For
    // each projection, gather and scatter tasks are constructed with the
    // correct indices. This happens in two phases.
    //   A) Owners are assigned to the faces, and the buffers are measured for size
    //   B) The tasks are generated, with correct indices
    // 2. For all the projections that are being processed by the local rank,
    // gather the task together in lists (`scatters`, `gathers`). These tasks
    // are grouped by responsible rank.
    // 3. Communicate the gather and scatter tasks using a queue to the
    // responsible ranks.
    // 4. Read out the queue, and gather the tasks in a vector.

    // tasks generated by the processing of the local projections.
    auto scatters = std::vector<std::vector<scatter_tasks>>(p);
    auto gathers = std::vector<std::vector<gather_tasks>>(p);

    // owners are assigned randomly, so we construct an engine.
    auto rd = std::random_device();
    auto engine = std::mt19937(rd());

    // PHASE A: Assign owners, and measure buffers
    // process projections in a round-robin fashion.
    for (int i = s; i < g.projection_count(); i += p) {
        auto pi = g.get_projection(i);

        // get faces for the i-th projection
        auto overlay = get_overlay_for_projection(pi, root, v);
        auto faces = get_faces(pi, overlay);

        // I think we need to:
        // - remember the owner
        // - count the total buffer size
        // - be able to traverse the faces list again, without rerunning
        //   the previous algorithm
        for (auto& face : faces) {
            // assign a random owner to the face off the list of processors
            auto owner = face.contributors[engine() % face.contributors.size()];

            // prepare gather_tasks for contributors, scatter_tasks for owner
            for (auto t : face.contributors) {
              gathers[t].push_back(gather_task{...});
                scatters[t].push_back(scatter_task{...});
            }
        }
    }

    // PHASE B: Construct tasks

    // auto sq = bulk::queue<scatter_task>(world);

    // for (int t = 0; t < p; ++t) {
    //     for (auto task : scatters) {
    //         sq(t).send(task)
    //     }
    // }

    // world.sync();
}

} // namespace pleiades
