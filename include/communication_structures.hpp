/*
 * TODO Global to local indices
 * TODO Remote indices per scanline for each target
 */

#include <random>
#include <vector>

#include <bulk/bulk.hpp>

namespace pleiades {

// convert a global index to a local one
std::size_t global_to_local(geometry g, std::size_t global, int rank) {
    return 0;
}

// a scanlines is a consecutive subarray from position `begin` up to `begin +
// count`
struct scanline {
    std::size_t begin;
    std::size_t count;
};

// a face has a list of contributing processors, and a list of scanlines
struct face {
    std::vector<int> contributors;
    std::vector<scanline> scanlines;
};

// for the gather step, one of the contributors of a face is considered its
// owner. the scanlines in `lines` are to be communicated to the owner's
// 'reduction buffer' at the index indicated by the first component of the pair
struct gather_task {
    int owner;
    std::vector<std::pair<std::size_t, scanline>> lines;
};

// for the scatter step, the owner communicates the reduction result back to the
// contributors. Each line has an associated tag, which is a list of remote
// indices of the contributors in their main data buffer
// TODO here the scanline indices are in the gather result buffer
struct scatter_task {
    std::vector<int> contributors;
    std::vector<std::pair<std::vector<std::size_t>, scanline>> lines;
};

template <typename T>
void gather(bulk::coarray<T> buffer, std::vector<gather_task> tasks,
            const T* data) {
    for (auto task : tasks) {
        for (auto [remote, line] : task.lines) {
            auto [begin, count] = line;
            buffer(task.owner)[{remote, remote + count}] = {&data[begin],
                                                            count};
        }
    }
    buffer.world.sync();
}

template <typename T>
void scatter(bulk::coarray<T> buffer, std::vector<scatter_task> tasks,
             const T* data) {
    for (auto task : tasks) {
        for (auto [remotes, line] : task.lines) {
            for (auto i = 0; i < task.contributors.size(); ++i) {
                auto remote = remotes[i];
                buffer(task.contributors[i])[{remote, remote + count}] = {
                    &data[begin], count};
            }
        }
        buffer.world.sync();
    }
}

/** Outputs the gather and scatter tasks for the local processor */
template <typename T>
std::pair<std::vector<gather_task>, std::vector<scatter_task>>
tasks(bulk::world& world, const tpt::geometry::base<3_D, T>& g,
      const tpt::grcb::node<T>& root, tpt::grcb::cube<T> v) {
    // alias local rank and number of processors
    auto s = world.rank();
    auto p = world.active_processors();

    // TODO how to deal with the dynamic buffer size? I think we require another
    // synchronization step

    // The strategy is as follows.
    // 1. Assign the projections round robin, and treat them independently. For
    // each projection, gather and scatter tasks are constructed with the
    // correct indices. This happens in two phases.
    //   A) Owners are assigned to the faces, and the buffers are measured for
    //   size B) The tasks are generated, with correct indices
    // 2. For all the projections that are being processed by the local rank,
    // gather the task together in lists (`scatters`, `gathers`). These tasks
    // are grouped by responsible rank.
    // 3. Communicate the gather and scatter tasks using a queue to the
    // responsible ranks.
    // 4. Read out the queue, and gather the tasks in a vector.

    // tasks generated by the processing of the local projections.
    auto scatters = std::vector<std::vector<scatter_tasks>>(p);
    auto gathers = std::vector<std::vector<gather_tasks>>(p);

    // owners are assigned randomly, so we construct an engine.
    auto rd = std::random_device();
    auto engine = std::mt19937(rd());

    // cache faces of local projections, and the assigned owners
    auto faces = std::vector<std::vector<face>>();
    auto owners = std::vector<std::vector<int>>();

    // the buffer size that we require for each processor
    auto B = std::vector<std::size_t>(p, 0);

    // PHASE A: 'Dry run': assign owners, and measure buffers
    // process projections in a round-robin fashion
    // Here, i is the local index, and J is the global index
    for (auto i = 0, auto J = s; J < g.projection_count(); J += p, i += 1) {
        auto pi = g.get_projection(J);

        // get faces for the i-th projection
        auto overlay = get_overlay_for_projection(pi, root, v);
        faces.push_back(get_faces(pi, overlay));

        // make zero-initialized list of owners
        owners.push_back(std::vector<int>(0, faces[i].size()));

        // We need to:
        // - assign and cache the owner
        // - count the total buffer size
        auto f = 0;
        for (auto& face : faces) {
            // assign a random owner to the face from the list of contributors
            owners[i][f] =
                face.contributors[engine() % face.contributors.size()];

            // measure the number of pixels
            auto pixels = std::accumulate(
                face.scanlines.begin(), face.scanlines.end(),
                [](auto total, auto [begin, count]) { return total + count; });

            B[t] += face.contributors.size() * pixels;

            f += 1;
        }
    }

    // now we have B[t], the local buffer offsets can be computed this is a full
    // p^2-relation, after this communication step we can compute partial sums.
    auto C = bulk::coarray<std::size_t>(world, p * p);
    for (auto u = 0; u < p; ++u) {
        for (auto t = 0; t < p; ++t) {
            C(u)[t * p + s] = B[t];
        }
    }
    world.sync();

    // D[t] is the beginning of our buffer portion in processor t
    auto D = std::vector<int>(p, 0);
    for (int t = 0; t < p; ++t) {
        for (int u = 0; u < s; ++u) {
            D[t] += C[t * p + u];
        }
    }

    // PHASE B: Construct tasks
    // prepare gather_tasks for contributors, scatter_tasks for owner
    for (auto i = 0, auto J = s; J < g.projection_count(); J += p, i += 1) {
        auto f = 0;
        for (auto& face : faces) {
            // TODO juggle indices
            auto t = owners[i][f];
            std::vector<std::size_t> scatter_offsets(face.contributors.size());
            for (auto [begin, count] : face.scanlines) {
                for (auto u : face.contributors) {
                    gathers[u].push_back({t, {D[t], {...}}});
                }
            }

            scatters[t].push_back(scatter_task{...});
            f += 1;
        }
    }

    // PHASE C: Distribute all tasks
    auto gq = bulk::queue<gather_task>(world);
    auto sq = bulk::queue<scatter_task>(world);

    for (int t = 0; t < p; ++t) {
        for (auto task : scatters[t]) {
            sq(t).send(task)
        }
        for (auto task : gathers[t]) {
            gq(t).send(task)
        }
    }

    // (this would be nice syntax, can we do something like this in Bulk)
    // sq(0..t..p).send*(scatters[t]);
    // gq(0..t..p).send*(gathers[t]);

    world.sync();

    gathers.resize(gq.size());
    scatters.resize(sq.size());
    std::copy(gq.begin(), gq.end(), gathers.begin());
    std::copy(sq.begin(), sq.end(), scatters.begin());

    return {gathers, scatters};
}

} // namespace pleiades
