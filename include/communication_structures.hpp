/*
 * TODO Global to local indices
 * DONE Remote indices per scanline for each target
 */

#include <functional>
#include <random>
#include <vector>

#include <bulk/bulk.hpp>

namespace pleiades {

// I assume pixel coordinates are in 'matrix access form'
struct geometry_info {
    // the number of projections in the geometry
    int projection_count;

    // the shape of the global projections
    std::pair<int, int> shape;

    // corner[t][i]: the upper left corner of projection i on processor t
    std::vector<std::vector<std::pair<int, int>>> corner;

    // columns[t]: the number of columns in the projections on processor t
    std::vector<int> columns;

    // offsets[t][i]: the offset of the i-th projection in the projection buffer
    // on processor t
    std::vector<std::vector<std::size_t>> offsets;
};

// convert a global index to a local one
// TODO: what parameters do we need
// t: target rank
// i: projection id
// j: flattened global pixel in the projection
std::size_t localize(geometry_info g, int t, int i, std::size_t j) {
    // global pixel
    auto u = j % std::get<0>(g.shape);
    auto v = j / std::get<0>(g.shape);

    // local pixel
    auto a = u - std::get<0>(g.corner[t][i]);
    auto b = v - std::get<1>(g.corner[t][i]);

    // local coordinate
    return g.offsets[t][i] + a * g.columns[t] + b;
}

// a scanlines is a consecutive subarray from position `begin` up to `begin +
// count`
struct scanline {
    std::size_t begin;
    std::size_t count;
};

// a face has a list of contributing processors, and a list of scanlines
struct face {
    std::vector<int> contributors;
    std::vector<scanline> scanlines;
};

// for the gather step, one of the contributors of a face is considered its
// owner. the scanlines in `lines` are to be communicated to the owner's
// 'reduction buffer' at the index indicated by the first component of the pair
struct gather_task {
    int owner;
    std::vector<std::pair<std::size_t, scanline>> lines;
};

// for the scatter step, the owner communicates the reduction result back to the
// contributors. Each line has an associated tag, which is a list of remote
// indices of the contributors in their main data buffer
// TODO here the scanline indices are in the gather result buffer
struct scatter_task {
    std::vector<int> contributors;
    std::vector<std::pair<std::vector<std::size_t>, scanline>> lines;
};

template <typename T>
void gather(bulk::coarray<T> buffer, std::vector<gather_task> tasks,
            const T* data) {
    for (auto task : tasks) {
        for (auto [remote, line] : task.lines) {
            auto [begin, count] = line;
            buffer(task.owner)[{remote, remote + count}] = {&data[begin],
                                                            count};
        }
    }
    buffer.world.sync();
}

template <typename T>
void scatter(bulk::coarray<T> buffer, std::vector<scatter_task> tasks,
             const T* data) {
    for (auto task : tasks) {
        for (auto [remotes, line] : task.lines) {
            for (auto i = 0u; i < task.contributors.size(); ++i) {
                auto remote = remotes[i];
                buffer(task.contributors[i])[{remote, remote + count}] = {
                    &data[begin], count};
            }
        }
        buffer.world.sync();
    }
}

/** Outputs the gather and scatter tasks for the local processor */
template <typename T>
std::pair<std::vector<gather_task>, std::vector<scatter_task>>
tasks(bulk::world& world, const tpt::geometry::base<3_D, T>& g,
      const tpt::grcb::node<T>& root, tpt::grcb::cube<T> v) {
    // alias local rank and number of processors
    auto s = world.rank();
    auto p = world.active_processors();

    // The strategy is as follows.
    // 1. Assign the projections round robin, and treat them independently. For
    // each projection, gather and scatter tasks are constructed with the
    // correct indices. This happens in two phases.
    //   A) Owners are assigned to the faces, and the buffers are measured for
    //   size B) The tasks are generated, with correct indices
    // 2. For all the projections that are being processed by the local rank,
    // gather the task together in lists (`scatters`, `gathers`). These tasks
    // are grouped by responsible rank.
    // 3. Communicate the gather and scatter tasks using a queue to the
    // responsible ranks.
    // 4. Read out the queue, and gather the tasks in a vector.
    //
    // The gather tasks correspond to a reduction operations, that add up the
    // local scanlines. We assume that the result of this is written into the
    // main 'projection data buffer', and the scatter operation also directly
    // writes to this 'projection data buffer'. This buffer should be on the
    // CPU, and registered using Bulk to enable communication optimizations.

    // tasks generated by the processing of the local projections.
    auto scatters = std::vector<std::vector<scatter_tasks>>(p);
    auto gathers = std::vector<std::vector<gather_tasks>>(p);

    // owners are assigned randomly, so we construct an engine.
    auto rd = std::random_device();
    auto engine = std::mt19937(rd());

    // cache faces of local projections, and the assigned owners
    auto faces = std::vector<std::vector<face>>();
    auto owners = std::vector<std::vector<int>>();

    // the buffer size that we require for each processor
    auto B = std::vector<std::size_t>(p, 0);

    // PHASE A: 'Dry run': assign owners, and measure buffers
    // process projections in a round-robin fashion
    // Here, i is the local index, and J is the global index
    for (auto i = 0, auto J = s; J < g.projection_count(); J += p, i += 1) {
        auto pi = g.get_projection(J);

        // get faces for the i-th projection
        auto overlay = get_overlay_for_projection(pi, root, v);
        faces.push_back(get_faces(pi, overlay));

        // make zero-initialized list of owners
        owners.push_back(std::vector<int>(0, faces[i].size()));

        // We need to:
        // - assign and cache the owner
        // - count the total buffer size
        auto f = 0;
        for (auto& face : faces) {
            // assign a random owner to the face from the list of contributors
            owners[i][f] =
                face.contributors[engine() % face.contributors.size()];

            // measure the number of pixels
            auto pixels = std::accumulate(
                face.scanlines.begin(), face.scanlines.end(),
                [](auto total, auto [begin, count]) { return total + count; });

            B[t] += face.contributors.size() * pixels;

            f += 1;
        }
    }

    // now we have B[t], the local buffer offsets can be computed this is a full
    // p^2-relation, after this communication step we can compute partial sums.
    auto C = bulk::coarray<std::size_t>(world, p * p);
    for (auto u = 0; u < p; ++u) {
        for (auto t = 0; t < p; ++t) {
            C(u)[t * p + s] = B[t];
        }
    }
    world.sync();

    // D[t] is the beginning of our gather buffer portion in processor t
    auto D = std::vector<int>(p, 0);
    for (int t = 0; t < p; ++t) {
        for (int u = 0; u < s; ++u) {
            D[t] += C[t * p + u];
        }
    }

    // PHASE B: Construct tasks
    // prepare gather_tasks for contributors, scatter_tasks for owner
    for (auto i = 0, auto J = s; J < g.projection_count(); J += p, i += 1) {
        auto f = 0;
        for (auto& face : faces) {
            // Juggle indices
            auto t = owners[i][f];

            // TODO try to avoid reallocation
            auto scanline_offsets = std::vector<std::vector<std::size_t>>(
                face.scanlines.size(),
                std::vector<std::size_t>(face.contributors().size()));

            for (auto [begin, count] : face.scanlines) {
                // TODO try to avoid reallocation
                auto begins =
                    std::vector<std::size_t>(face.contributors.size());

                for (auto u : face.contributors) {
                    gathers[u].push_back(
                        {t, {D[t], {localize(u, begin), count}}});
                    D[t] += count;
                    begins.push_back(localize(u, begin));
                }

                scatters[t].push_back(
                    {face.contributors, {begins, {localize(t, begin), count}}});
            }

            f += 1;
        }
    }

    // PHASE C: Distribute all tasks
    auto gq = bulk::queue<gather_task>(world);
    auto sq = bulk::queue<scatter_task>(world);

    for (int t = 0; t < p; ++t) {
        for (auto task : scatters[t]) {
            sq(t).send(task)
        }
        for (auto task : gathers[t]) {
            gq(t).send(task)
        }
    }

    // (this would be nice syntax, can we do something like this in Bulk)
    // sq(0..t..p).send*(scatters[t]);
    // gq(0..t..p).send*(gathers[t]);

    world.sync();

    gathers.resize(gq.size());
    scatters.resize(sq.size());
    std::copy(gq.begin(), gq.end(), gathers.begin());
    std::copy(sq.begin(), sq.end(), scatters.begin());

    return {gathers, scatters};
}

} // namespace pleiades
